---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---
# Tabular Q learning

Initialization
We have 10 states and 3 actions (Transfer from station 1 to 2; Transfer from station 2 to 1; Do nothing)
```{r}
num_state <- 11
num_action <- 3
gamma <- 0.9 #reward decay
alpha <- 0.1 #learning rate
max_step <- 10000
prob_1 <- 0.6
prob_2 <- 0.4
```

Assume in total there are 10 people who want to rent a car at every step
Each arrival follows a Bernoulli distribution, the total amount of arrival (out of 10) follows binomial distribution.
Create a function that returns a random number that follows binomial distribution
```{r}
generateBinomial <- function(prob)
  return(rbinom(1, 10, prob))
```

Build a q table with num_state rows, and num_action columns
```{r}
build_q_table <- function(num_state, num_action)
  return(matrix(0, nrow=num_state, ncol=num_action))
```

Choose action
```{r}
choose_action <- function(state, Q_table, epsilon)
  return(ifelse(runif(1,min=0,max=1) > epsilon, match(max(Q_table[state,]),Q_table[state,]), sample(c(1,2,3),1)))
```

Update the environment based on the current state and the action chosen
```{r}
# (action[1]: Transfer from station 1 to 2; 
#  action[2]: Transfer from station 2 to 1; 
#  action[3]: Do nothing)
# state[n]: station 1 has n-1 cars
update_environment <- function(state, action) {
  if((state == 1 & action == 1) | (state == 11 & action == 2)) {
    return(state)
  } else {
    if(action == 1) {
      return(state-1)
    } else if(action == 2) {
      return(state+1)
    } else {
      return(state)
    }
  }
}
```

Obtain reward
```{r}
# obtain reward
get_reward <- function(x1, x2, new_state) {
  num_cars_1 <- new_state - 1
  num_cars_2 <- 10 - num_cars_1
  reward <- 0
  # If we do not have enough car, we should give a punishment
  if((x1 > num_cars_1) | (x2 > num_cars_2)) {
    reward <- -1 * max(abs(x1 - num_cars_1), abs(x2 - num_cars_2))
  } else {
    reward <- 1 
  }
  return(reward)
  }

```

Update Q table
```{r}
# update Q table
learn <- function(Q_table, alpha, gamma, state, action, reward, new_state) {
  Q_table[state, action] <- Q_table[state, action] + alpha*(reward + gamma * max(Q_table[new_state,]) - Q_table[state, action])
  return(Q_table)
}
```

Main loop
```{r}
step = 1
state = 10
epsilon <- 1
Q_table <- build_q_table(num_state, num_action)
inf_norm <- replicate(max_step, 0)
while(step < max_step) {
  step <- step + 1
  action <- choose_action(state, Q_table, epsilon)
  new_state <- update_environment(state, action)
  x1 <- generateBinomial(prob_1)
  x2 <- generateBinomial(prob_2)
  #x1 <- 6
  #x2 <- 4
  reward <- get_reward(x1, x2, new_state)
  Q_table <- learn(Q_table, alpha, gamma, state, action, reward, new_state)
  state <- new_state
  epsilon = epsilon - 1/max_step
}

i=1
while(i<11) {
  i=i+1
  idx = match(max(Q_table[i,]),Q_table[i,])
  if(idx == 1) {
    print("decrease car in station 1")
  } else if(idx == 2) {
    print("increase car in station 1")
  } else {
    print("do nothing")
  }
}
```

